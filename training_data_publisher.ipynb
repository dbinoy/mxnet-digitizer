{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logger = logging.getLogger('mxnet_data')\n",
    "fh = logging.FileHandler('data-publishing.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last(text, token):\n",
    "    position = []\n",
    "    for n in range(len(text)):\n",
    "        if text.find(token, n, n+1) != -1:\n",
    "            position.append(n)\n",
    "    return position[len(position)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_to_queue(sqs, queuename, filename):\n",
    "    data_file = open(filename, 'r')\n",
    "    data_list = data_file.readlines()\n",
    "    data_file.close()    \n",
    "    message = ''\n",
    "    messageDeduplicationId=filename[find_last(filename, '_')+1:find_last(filename, '.')]\n",
    "    messageGroupId=filename[find_last(filename, '/')+1:find_last(filename, '_')]\n",
    "    for record in data_list:\n",
    "        message = message + record\n",
    "    queue = sqs.get_queue_by_name(QueueName=queuename)\n",
    "    response = queue.send_message(\n",
    "        MessageBody=message,\n",
    "        MessageDeduplicationId=messageDeduplicationId,\n",
    "        MessageGroupId=messageGroupId\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqs = boto3.resource('sqs', 'us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = 'training-data-queue.fifo'\n",
    "logger.info('Publishing Queue - ' + queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/incremental/train_053.csv\n",
      "Created file : ./dataset/incremental/train_053.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-be347eb11452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_filename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfind_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfind_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0msmall_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./dataset/incremental/train_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlines_per_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lines_per_file = 120\n",
    "smallfile = None\n",
    "progress = 0\n",
    "if not os.path.exists('./dataset/incremental'):\n",
    "    os.makedirs('./dataset/incremental') \n",
    "if os.path.exists('check.dat'):\n",
    "    check = open('check.dat','r')\n",
    "    progress = int(check.readlines()[0])\n",
    "    check.close()\n",
    "with open('./dataset/train.csv') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        if int((lineno - 1) / lines_per_file) < progress:\n",
    "            continue\n",
    "        if (lineno - 1)  % lines_per_file == 0:\n",
    "            if smallfile:\n",
    "                smallfile.close()\n",
    "                print(\"Created file : \" + smallfile.name)\n",
    "                response = publish_to_queue(sqs,queue,small_filename)\n",
    "                logger.info('Data Packet - ' + small_filename[find_last(small_filename, '/')+1:find_last(small_filename, '.')]+ ' published at ' + response['ResponseMetadata']['HTTPHeaders']['date'])               \n",
    "                check = open('check.dat','w') \n",
    "                check.write(str(int(small_filename[find_last(small_filename, '_')+1:find_last(small_filename, '.')])))\n",
    "                check.close()\n",
    "                time.sleep(30)                \n",
    "            small_filename = './dataset/incremental/train_{}.csv'.format(str(int((lineno - 1) / lines_per_file) + 1).zfill(3))\n",
    "            print(small_filename)\n",
    "            smallfile = open(small_filename, \"w\")\n",
    "        if line.find(\"pixel\") < 0:\n",
    "            smallfile.write(line)\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "if os.path.exists('check.dat'):\n",
    "    os.remove('check.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
