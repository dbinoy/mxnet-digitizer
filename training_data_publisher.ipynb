{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logger = logging.getLogger('mxnet_data')\n",
    "fh = logging.FileHandler('data-publishing.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_queue_url(sqs, name):\n",
    "    response = sqs.list_queues()\n",
    "    for url in response['QueueUrls']:\n",
    "        if url.find('/' + name + '.fifo') != -1 or url.find('/' + name)!= -1:\n",
    "            return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_last(text, token):\n",
    "    position = []\n",
    "    for n in range(len(text)):\n",
    "        if text.find(token, n, n+1) != -1:\n",
    "            position.append(n)\n",
    "    return position[len(position)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def publish_to_queue(sqs, queue, filename):\n",
    "    data_file = open(filename, 'r')\n",
    "    data_list = data_file.readlines()\n",
    "    data_file.close()    \n",
    "    message = ''\n",
    "    for record in data_list:\n",
    "        message = message + record\n",
    "    response = sqs.send_message(\n",
    "        QueueUrl=queue,\n",
    "        MessageBody=message,\n",
    "        MessageDeduplicationId=filename[find_last(filename, '_')+1:find_last(filename, '.')],\n",
    "        MessageGroupId=filename[find_last(filename, '/')+1:find_last(filename, '_')]\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqs = boto3.client('sqs', 'us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = get_queue_url(sqs, 'training-data-queue')\n",
    "logger.info('Publishing Queue URL - ' + queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_per_file = 120\n",
    "smallfile = None\n",
    "progress = 0\n",
    "if os.path.exists('check.dat'):\n",
    "    check = open('check.dat','r')\n",
    "    progress = int(check.readlines()[0])\n",
    "    check.close()\n",
    "with open('./dataset/mnist_train.csv') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        if int(lineno / lines_per_file) < progress:\n",
    "            continue\n",
    "        if lineno % lines_per_file == 0:\n",
    "            if smallfile:\n",
    "                smallfile.close()\n",
    "                response = publish_to_queue(sqs,queue,small_filename)\n",
    "                logger.info('Data Packet - ' + small_filename[find_last(small_filename, '/')+1:find_last(small_filename, '.')]+ ' published at ' + response['ResponseMetadata']['HTTPHeaders']['date'])\n",
    "                os.remove(small_filename)\n",
    "                check = open('check.dat','w') \n",
    "                check.write(str(int(small_filename[find_last(small_filename, '_')+1:find_last(small_filename, '.')])))\n",
    "                check.close()\n",
    "                time.sleep(30)                \n",
    "            small_filename = './dataset/incremental/mnist_train_{}.csv'.format(str(int(lineno / lines_per_file) + 1).zfill(3))\n",
    "            smallfile = open(small_filename, \"w\")\n",
    "        smallfile.write(line)\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "if os.path.exists('check.dat'):\n",
    "    os.remove('check.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
