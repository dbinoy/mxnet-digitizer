{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import numpy as np\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logger = logging.getLogger('mxnet_training')\n",
    "fh = logging.FileHandler('model-training.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 120\n",
    "epoch = 30\n",
    "model_path = 'model'\n",
    "model_name = 'mxnet_cnn_digits'\n",
    "data_folder = \"./dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, filename, s3, model_bucket, logger):\n",
    "    if not os.path.isfile(path + '/' + filename) or not os.access(path + '/' + filename, os.R_OK):\n",
    "        s3.download_file(model_bucket, filename, path + '/' + filename)\n",
    "    data_file = open(path + '/'+filename, 'r')\n",
    "    data_list = data_file.readlines()\n",
    "    data_file.close()\n",
    "    logger.info(str(len(data_list)) + ' samples loaded from ' + path + '/'+filename)\n",
    "    labels = np.empty([len(data_list),])\n",
    "    pixels = np.empty([len(data_list),1,28,28])\n",
    "    index=0\n",
    "    for record in data_list:\n",
    "        all_values = record.split(',')\n",
    "        labels[index] = all_values[0]\n",
    "        pixel = np.asfarray(all_values[1:]).reshape(28,28).astype(np.float32)/255\n",
    "        pixels[index] = pixel\n",
    "        index = index + 1  \n",
    "    data = {'label': labels, 'data': pixels}    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_queue_url(sqs, name):\n",
    "    response = sqs.list_queues()\n",
    "    for url in response['QueueUrls']:\n",
    "        if url.find('/' + name + '.fifo') != -1 or url.find('/' + name)!= -1:\n",
    "            return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_messages(sqs, queue, logger):\n",
    "    messages = []\n",
    "    response = sqs.receive_message(\n",
    "        QueueUrl=queue,\n",
    "        AttributeNames=['MessageDeduplicationId', 'MessageGroupId', 'ReceiptHandle']\n",
    "    ) \n",
    "    if 'Messages' in response:\n",
    "        for msg in response['Messages']:\n",
    "            message = {\n",
    "                        'group': msg['Attributes']['MessageGroupId'], \n",
    "                        'id': msg['Attributes']['MessageDeduplicationId'], \n",
    "                        'data': msg['Body'],\n",
    "                        'handle': msg['ReceiptHandle']}\n",
    "            messages.append(message)\n",
    "            logger.info('Message fetched - group:' + msg['Attributes']['MessageGroupId'] + ', id: ' + msg['Attributes']['MessageDeduplicationId'])\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_message(sqs, queue, message):\n",
    "    response = sqs.delete_message(\n",
    "        QueueUrl=queue,\n",
    "        ReceiptHandle=message['handle']\n",
    "    ) \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(message, logger):\n",
    "    data_list = message['data'][0:len(message['data'])-1].split('\\n')\n",
    "    logger.info(str(len(data_list)) + ' samples loaded from data packet - ' + message['group'] + '_' + message['id'])\n",
    "    labels = np.empty([len(data_list),])\n",
    "    pixels = np.empty([len(data_list),1,28,28])\n",
    "    index=0\n",
    "    for record in data_list:\n",
    "        all_values = record.split(',') \n",
    "        labels[index] = all_values[0]\n",
    "        pixel = np.asfarray(all_values[1:]).reshape(28,28).astype(np.float32)/255\n",
    "        pixels[index] = pixel\n",
    "        index = index + 1  \n",
    "    data = {'label': labels, 'data': pixels}    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(batch_size, logger): \n",
    "    #Define Multilayer perceptron using MXNet symbolic interface\n",
    "\n",
    "    #Placeholder variable for input data\n",
    "    data = mx.sym.var('data')\n",
    "\n",
    "    # first conv layer\n",
    "    conv1 = mx.sym.Convolution(data=data, kernel=(5,5), num_filter=20)\n",
    "    tanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\n",
    "    pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "\n",
    "    # second conv layer\n",
    "    conv2 = mx.sym.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "    tanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\n",
    "    pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "\n",
    "    # first fullc layer\n",
    "    flatten = mx.sym.flatten(data=pool2)\n",
    "    fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "    tanh3 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n",
    "\n",
    "    # second fullc\n",
    "    fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=10)\n",
    "    # softmax loss\n",
    "    softmax = mx.sym.SoftmaxOutput(data=fc2, name='softmax')\n",
    "\n",
    "\n",
    "    # Visualize the network structure with output size\n",
    "    shape = {'data' : (batch_size, 1, 28, 28)}\n",
    "    #mx.viz.plot_network(symbol=lenet, shape=shape)\n",
    "    \n",
    "    model  = mx.mod.Module(symbol=softmax, context=mx.gpu())\n",
    "    \n",
    "    logger.info('Training model created')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, model_name, epoch, logger): \n",
    "    model = mx.mod.Module.load(model_path + '/' + str(epoch) + '/' + model_name, epoch, load_optimizer_states =True, context=mx.gpu())\n",
    "    logger.info('Training model '+model_name+' loaded from ' + model_path + '/' + str(epoch))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_accuracy(model, test_data, batch_size):    \n",
    "    #Test iteration to measure prediction accuracy\n",
    "    test_iter = mx.io.NDArrayIter(test_data['data'], test_data['label'], batch_size)\n",
    "    # predict accuracy of mlp\n",
    "    acc = mx.metric.Accuracy()\n",
    "    model.score(test_iter, acc)\n",
    "    #assert acc.get()[1] > 0.98       \n",
    "    return str(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(message, model, test_data, batch_size, epoch, logger):\n",
    "    train_data = prepare_data(message, logger)\n",
    "    train_iter = mx.io.NDArrayIter(train_data['data'], train_data['label'], batch_size, shuffle=True)\n",
    "    val_iter = mx.io.NDArrayIter(test_data['data'], test_data['label'], batch_size)\n",
    "    # Train for a maximum of num_epoch epochs, until convergence\n",
    "    model.fit(train_iter, \n",
    "                  eval_data=val_iter, \n",
    "                  optimizer='sgd', \n",
    "                  optimizer_params={'learning_rate':0.1}, \n",
    "                  eval_metric='acc',  \n",
    "                  batch_end_callback = mx.callback.Speedometer(batch_size, 100), \n",
    "                  num_epoch=epoch)      \n",
    "    \n",
    "    accuracy = measure_accuracy(model, test_data, batch_size)\n",
    "    logger.info('Accuracy after processing packet - ' + message['group'] + '_' + message['id'] + ' : ' + accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_new_model(message, test_data, batch_size, epoch, logger): \n",
    "    model = create_model(batch_size, logger)\n",
    "    train(message, model, test_data, batch_size, epoch, logger)\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_loaded_model(model_path, model_name, message, test_data, batch_size, epoch, logger): \n",
    "    model = load_model(model_path, model_name, epoch, logger)\n",
    "    train(message, model, test_data, batch_size, epoch, logger)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, model_path, model_name, epoch, s3, model_bucket, logger):\n",
    "    model.save_checkpoint(model_path + '/' + str(epoch) + '/' + model_name, epoch, save_optimizer_states=True)\n",
    "    logger.info('Training model '+ model_name +' saved at ' + model_path + '/' + str(epoch))   \n",
    "    model_files=[]\n",
    "    for (dirpath, dirnames, filenames) in os.walk(model_path + '/' + str(epoch)):\n",
    "        model_files.extend(filenames)\n",
    "        break\n",
    "    for model_file in model_files:\n",
    "        s3.upload_file(model_path + '/' + str(epoch) + '/' + model_file, model_bucket, model_file)\n",
    "        logger.info('Training model file '+ model_file +' uploaded to S3 Bucket ' + model_bucket)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_model(model, model_path, model_name, epoch, s3, model_bucket, logger):\n",
    "    epoch_name = model_name + '-' + epoch.zfill(4)\n",
    "    params_file = epoch_name + '.params'\n",
    "    states_file = epoch_name + '.states'\n",
    "    json_file = model_name + '-symbol.json'\n",
    "    fetched = True\n",
    "    if not os.path.exists(model_path + '/' + str(epoch)):\n",
    "        os.makedirs(model_path + '/' + str(epoch))        \n",
    "    try:\n",
    "        if not os.path.exists(model_path + '/' + str(epoch) + '/' + params_file):\n",
    "            s3.download_file(model_bucket, params_file, model_path + '/' + str(epoch) + '/' + params_file)    \n",
    "        if not os.path.exists(model_path + '/' + str(epoch) + '/' + states_file):\n",
    "            s3.download_file(model_bucket, states_file, model_path + '/' + str(epoch) + '/' + states_file) \n",
    "        if not os.path.exists(model_path + '/' + str(epoch) + '/' + json_file):\n",
    "            s3.download_file(model_bucket, json_file, model_path + '/' + str(epoch) + '/' + json_file) \n",
    "        logger.info('Training model '+ model_name +' ready to be loaded')\n",
    "    except:\n",
    "        logger.info('Training model '+ model_name +' does not exist in locally or in S3 Bucket ' + model_bucket)\n",
    "        fetched = False\n",
    "    return fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqs = boto3.client('sqs', 'us-east-1')\n",
    "s3 = boto3.client('s3')\n",
    "queue = get_queue_url(sqs, 'training-data-queue')\n",
    "test_data = load_data('dataset', 'mnist_test.csv', s3, 'my-ml-data-set', logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exists = fetch_model(model, model_path, model_name, epoch, s3, model_bucket, logger)  \n",
    "while (True):\n",
    "    messages = get_messages(sqs, queue, logger)\n",
    "    if len(messages) > 0 :\n",
    "        logger.info(str(len(messages)) + ' data packets found, model being trained...')      \n",
    "        for message in messages:\n",
    "            if exists:\n",
    "                model = train_loaded_model(model_path, model_name, message, test_data, batch_size, epoch, logger)                \n",
    "            else:       \n",
    "                model = train_new_model(message, test_data, batch_size, epoch, logger)\n",
    "                exists = True\n",
    "            save_model(model,model_path, model_name, epoch, s3, 'my-ml-data-set', logger)    \n",
    "            delete_message(sqs, queue, message)        \n",
    "    else:\n",
    "        logger.info('No data packets found, will check back in a moment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
